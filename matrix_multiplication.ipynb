{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZIDL8M-zo8a"
      },
      "outputs": [],
      "source": [
        "# Install the nvcc4jupyter plugin\n",
        "!pip install nvcc4jupyter\n",
        "\n",
        "# Load the extension into the notebook\n",
        "%load_ext nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int nDevices;\n",
        "    cudaGetDeviceCount(&nDevices);\n",
        "    for (int i = 0; i < nDevices; i++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "        printf(\"Device Number: %d\\n\", i);\n",
        "        printf(\"  Device name: %s\\n\", prop.name);\n",
        "        printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "        printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "        printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "               2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    }\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "yZIkXeUPzxhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath>\n",
        "\n",
        "// Utility function to handle CUDA errors; use this after every CUDA call\n",
        "inline cudaError_t checkCudaErr(cudaError_t result, char const *const func, const int line) {\n",
        "    if (result != cudaSuccess) {\n",
        "        std::cerr << \"CUDA error = \" << static_cast<int>(result) << \" at \" <<\n",
        "        func << \":\" << line << \" '\" << cudaGetErrorString(result) << \"'\" << std::endl;\n",
        "        exit(1);\n",
        "    }\n",
        "    return result;\n",
        "}\n",
        "#define CUDA_CHECK(val) checkCudaErr((val), __func__, __LINE__)\n",
        "\n",
        "// Kernel declarations (as provided earlier)\n",
        "__global__ void matmul_rec_glob(float* A, float* B, float* C, int n, int k, int m) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < n && col < m) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < k; ++i) {\n",
        "            sum += A[row * k + i] * B[i * m + col];\n",
        "        }\n",
        "        C[row * m + col] = sum;\n",
        "    }\n",
        "}\n",
        "__global__ void matmul_rec_shar(float* A, float* B, float* C, int n, int k, int m) {\n",
        "    int bx = blockIdx.x, by = blockIdx.y;\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    __shared__ float sA[32][32]; // Tile size of 32x32\n",
        "    __shared__ float sB[32][32];\n",
        "\n",
        "    for (int t = 0; t < (k + 31) / 32; ++t) {\n",
        "        if (row < n && (t * 32 + tx) < k)\n",
        "            sA[ty][tx] = A[row * k + t * 32 + tx];\n",
        "        else\n",
        "            sA[ty][tx] = 0.0;\n",
        "\n",
        "        if (col < m && (t * 32 + ty) < k)\n",
        "            sB[ty][tx] = B[(t * 32 + ty) * m + col];\n",
        "        else\n",
        "            sB[ty][tx] = 0.0;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int i = 0; i < 32; ++i) {\n",
        "            sum += sA[ty][i] * sB[i][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (row < n && col < m)\n",
        "        C[row * m + col] = sum;\n",
        "}\n",
        "\n",
        "\n",
        "// Function to initialize matrices with some values\n",
        "void initializeMatrix(float* matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows * cols; i++) {\n",
        "        matrix[i] = static_cast<float>(rand()) / static_cast<float>(RAND_MAX);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 640, k = 480, m = 800; // Example dimensions\n",
        "    size_t sizeA = n * k * sizeof(float);\n",
        "    size_t sizeB = k * m * sizeof(float);\n",
        "    size_t sizeC = n * m * sizeof(float);\n",
        "\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate host memory\n",
        "    h_A = (float*)malloc(sizeA);\n",
        "    h_B = (float*)malloc(sizeB);\n",
        "    h_C = (float*)malloc(sizeC);\n",
        "\n",
        "    // Initialize matrices\n",
        "    initializeMatrix(h_A, n, k);\n",
        "    initializeMatrix(h_B, k, m);\n",
        "\n",
        "    // Allocate device memory\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_A, sizeA));\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_B, sizeB));\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_C, sizeC));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeA, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_B, h_B, sizeB, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Setup execution parameters\n",
        "    dim3 threadsPerBlock(32, 32);\n",
        "    dim3 blocksPerGrid((m + threadsPerBlock.x - 1) / threadsPerBlock.x, (n + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch kernels\n",
        "    matmul_rec_glob<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n, k, m);\n",
        "    CUDA_CHECK(cudaPeekAtLastError());  // Check for any errors launching the kernel\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());  // Sync after completion\n",
        "\n",
        "    // Launch the shared memory version\n",
        "    matmul_rec_shar<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n, k, m);\n",
        "    CUDA_CHECK(cudaPeekAtLastError());  // Check for any errors launching the kernel\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());  // Sync after completion\n",
        "\n",
        "    // Copy result matrix back to host\n",
        "    CUDA_CHECK(cudaMemcpy(h_C, d_C, sizeC, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Check results (simplified check)\n",
        "    for (int i = 0; i < n * m; i++) {\n",
        "        if (fabs(h_C[i]) > 1e-5) {\n",
        "            std::cout << \"Result verification failed at element \" << i << \": \" << h_C[i] << std::endl;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    CUDA_CHECK(cudaFree(d_A));\n",
        "    CUDA_CHECK(cudaFree(d_B));\n",
        "    CUDA_CHECK(cudaFree(d_C));\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "wun1d0SL4l28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBuvqonN4tKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}