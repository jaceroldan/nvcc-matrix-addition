{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the nvcc4jupyter plugin\n",
        "!pip install nvcc4jupyter\n",
        "\n",
        "# Load the extension into the notebook\n",
        "%load_ext nvcc4jupyter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrMUb7gXjOPc",
        "outputId": "89f6f9af-8f6f-47bf-a74d-cd4039eaa5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpqcsmzpb7\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello(){\n",
        "    printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    hello<<<2, 2>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4hoRVWUjkda",
        "outputId": "2c40c808-cb7e-4a11-bb9b-44ed17c28ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block: 0, thread: 0\n",
            "Hello from block: 0, thread: 1\n",
            "Hello from block: 1, thread: 0\n",
            "Hello from block: 1, thread: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOFwk2AQbrRT",
        "outputId": "37aeda39-db07-4d5d-85b7-ab9f334c4fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int nDevices;\n",
        "    cudaGetDeviceCount(&nDevices);\n",
        "    for (int i = 0; i < nDevices; i++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "        printf(\"Device Number: %d\\n\", i);\n",
        "        printf(\"  Device name: %s\\n\", prop.name);\n",
        "        printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "        printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "        printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "               2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    }\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1e(float *A, float *B, float *C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int index = row * N + col;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        A[index] = B[index] + C[index];\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1024;  // example size\n",
        "    size_t size = N * N * sizeof(float);\n",
        "    float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate space for host copies and setup values\n",
        "    A = (float *)malloc(size); randomInit(A, N*N);\n",
        "    B = (float *)malloc(size); randomInit(B, N*N);\n",
        "    C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "    // Allocate space for device copies\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    // Copy inputs to device\n",
        "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel_1t1e() kernel on GPU\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x, (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "    kernel_1t1e<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(A); free(B); free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "NQu60AoNmWiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1r(float *A, float *B, float *C, int N) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N) { // Ensure the row is within the matrix\n",
        "        int index = row * N;\n",
        "        for (int col = 0; col < N; col++) {\n",
        "            A[index + col] = B[index + col] + C[index + col];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1024;  // example matrix size\n",
        "    size_t size = N * N * sizeof(float);\n",
        "    float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate space for host copies of A, B, C and setup values\n",
        "    A = (float *)malloc(size); randomInit(A, N*N);\n",
        "    B = (float *)malloc(size); randomInit(B, N*N);\n",
        "    C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "    // Allocate space for device copies of A, B, C\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    // Copy inputs to device\n",
        "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel_1t1r() kernel on GPU\n",
        "    dim3 threadsPerBlock(1);\n",
        "    dim3 numBlocks(N);\n",
        "    kernel_1t1r<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(A); free(B); free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "fwtnDguLnsKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1c(float *A, float *B, float *C, int N) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (col < N) { // Ensure the column is within the matrix\n",
        "        for (int row = 0; row < N; row++) {\n",
        "            int index = row * N + col;\n",
        "            A[index] = B[index] + C[index];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1024;  // example matrix size\n",
        "    size_t size = N * N * sizeof(float);\n",
        "    float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate space for host copies of A, B, C and setup values\n",
        "    A = (float *)malloc(size); randomInit(A, N*N);\n",
        "    B = (float *)malloc(size); randomInit(B, N*N);\n",
        "    C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "    // Allocate space for device copies of A, B, C\n",
        "    cudaMalloc((void **)&d_A, size);\n",
        "    cudaMalloc((void **)&d_B, size);\n",
        "    cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "    // Copy inputs to device\n",
        "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel_1t1c() kernel on GPU\n",
        "    dim3 threadsPerBlock(1);\n",
        "    dim3 numBlocks(N);\n",
        "    kernel_1t1c<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(A); free(B); free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "lSm3wHMptII0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}