{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the nvcc4jupyter plugin\n",
        "!pip install nvcc4jupyter\n",
        "\n",
        "# Load the extension into the notebook\n",
        "%load_ext nvcc4jupyter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrMUb7gXjOPc",
        "outputId": "955fa391-5647-46f2-da6c-0f9ea8f70323"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpsra69gdy\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOFwk2AQbrRT",
        "outputId": "29b28116-89d3-4934-c4c9-a49cfcf6c54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int nDevices;\n",
        "    cudaGetDeviceCount(&nDevices);\n",
        "    for (int i = 0; i < nDevices; i++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "        printf(\"Device Number: %d\\n\", i);\n",
        "        printf(\"  Device name: %s\\n\", prop.name);\n",
        "        printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "        printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "        printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "               2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    }\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1e(float *A, float *B, float *C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int index = row * N + col;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        A[index] = B[index] + C[index];\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizes[] = {4, 16, 64, 256, 1024, 4096, 16384}; // Matrix sizes to test\n",
        "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
        "    int numRuns = 10;\n",
        "\n",
        "    for (int s = 0; s < numSizes; s++) {\n",
        "        int N = sizes[s];\n",
        "        size_t size = N * N * sizeof(float);\n",
        "        float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "        float totalMilliseconds = 0.0, avgMilliseconds;\n",
        "\n",
        "        // Allocate space for host copies of A, B, C and setup values\n",
        "        A = (float *)malloc(size); randomInit(A, N*N);\n",
        "        B = (float *)malloc(size); randomInit(B, N*N);\n",
        "        C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "        // Allocate space for device copies of A, B, C\n",
        "        cudaMalloc((void **)&d_A, size);\n",
        "        cudaMalloc((void **)&d_B, size);\n",
        "        cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "        // Copy inputs to device\n",
        "        cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Setup CUDA events for timing\n",
        "        cudaEvent_t start, end;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&end);\n",
        "\n",
        "        for (int i = 0; i < numRuns; i++) {\n",
        "            // Record event and launch kernel\n",
        "            cudaEventRecord(start);\n",
        "\n",
        "            // Launch kernel_1t1c() kernel on GPU\n",
        "            dim3 threadsPerBlock(1);\n",
        "            dim3 numBlocks(N);\n",
        "            kernel_1t1e<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "            // Stop recording\n",
        "            cudaEventRecord(end);\n",
        "            cudaEventSynchronize(end); // Wait for the stop event to complete\n",
        "\n",
        "            float milliseconds = 0;\n",
        "\n",
        "            // Calculate elapsed time\n",
        "            cudaEventElapsedTime(&milliseconds, start, end);\n",
        "            totalMilliseconds += milliseconds;\n",
        "        }\n",
        "\n",
        "        avgMilliseconds = totalMilliseconds / numRuns;\n",
        "        printf(\"N = %d: Average Time for kernel_1t1c over %d runs: %f ms\\n\", N, numRuns, avgMilliseconds);\n",
        "\n",
        "        // Wait for GPU to finish before accessing on host\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Print results or check output\n",
        "        printf(\"First 10 elements for N = %d:\\n\", N);\n",
        "        for (int i = 0; i < 10 && i < N; i++) { // Example of printing first 10 elements\n",
        "            printf(\"%f \", A[i]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "\n",
        "        // Cleanup\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(end);\n",
        "        cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "        free(A); free(B); free(C);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "NQu60AoNmWiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27617342-22c4-4d4a-8cdc-c09ee295cf88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 4: Average Time for kernel_1t1c over 10 runs: 0.023427 ms\n",
            "First 10 elements for N = 4:\n",
            "124.835159 101.332863 77.915482 113.125603 \n",
            "N = 16: Average Time for kernel_1t1c over 10 runs: 0.006826 ms\n",
            "First 10 elements for N = 16:\n",
            "74.517647 98.296608 18.021336 99.150787 143.830017 130.534576 104.634445 129.568481 97.333878 128.621811 \n",
            "N = 64: Average Time for kernel_1t1c over 10 runs: 0.006458 ms\n",
            "First 10 elements for N = 64:\n",
            "51.953617 89.961128 61.007092 38.918518 170.510712 138.459351 109.247101 99.387833 164.447388 99.482315 \n",
            "N = 256: Average Time for kernel_1t1c over 10 runs: 0.007344 ms\n",
            "First 10 elements for N = 256:\n",
            "40.772362 65.722710 128.192261 50.025650 61.464783 177.513412 120.943428 73.261261 154.609848 95.283554 \n",
            "N = 1024: Average Time for kernel_1t1c over 10 runs: 0.009325 ms\n",
            "First 10 elements for N = 1024:\n",
            "136.829529 122.516586 98.160301 84.390724 27.842970 93.901207 71.524582 172.090012 108.210281 102.350830 \n",
            "N = 4096: Average Time for kernel_1t1c over 10 runs: 0.017562 ms\n",
            "First 10 elements for N = 4096:\n",
            "64.705223 80.196503 103.708237 97.520508 100.835884 122.165680 100.934860 66.719604 95.009132 35.037598 \n",
            "N = 16384: Average Time for kernel_1t1c over 10 runs: 0.051632 ms\n",
            "First 10 elements for N = 16384:\n",
            "114.099922 113.655663 68.049171 76.889755 121.218437 21.092766 64.133835 69.955887 84.861740 133.791931 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of each kernel\n",
        "\n",
        "## `kernel_1t1e` (One Thread per Element)\n",
        "\n",
        "This kernel is designed to perform addition on two matrices, where each thread in the grid has to add a single element from two input matrices B and C, and store the result in the corresponding element of the output matrix A. This design allows the kernel to fully utilize the GPU's parallel processing capabilities by assigning one thread to each operation, which is straightforward and maximizes parallelism at the element level.\n",
        "\n",
        "### Key characteristics\n",
        "* **Simple**: Each thread handles a single element, making the program logic straightforward.\n",
        "* **Scalable**: This approach is robust enough to handle effectively and efficiently the size of the matrices given that there are enough GPU resources.\n",
        "\n",
        "### Launch configuration\n",
        "\n",
        "`kernel_1t1e` involves setting up a grid of threads such that there is a thread for each element of the corresponding matrix operands.\n",
        "\n",
        "* **Threads per block**: These are 2-dimensional, and in all of the test runs to `kernel_1t1e`, this was set as 16x16, which fits in well with our constraint of maximum threads per block limit. This is well suited also for this kind of memory access pattern, as we will discuss below.\n",
        "* **Blocks per grid**. This is calculated for covering the whole matrix, with the number of blocks in each grid dimension set as $\\lceil N/threads \\rceil$, where $N$ is the dimension of the matrix.\n",
        "\n",
        "### Memory access pattern\n",
        "\n",
        "The function `kernel_1t1e` benefits from coalesced memory access pattern when each thread access consecutive elements of the matrices. In a row-major storage order, if each thread access an element that lies consecutively in memory (e.g. elements of a row when accessed by threads in a warp), the memory transactions are coalesced into as few as possible, which maximizes memory throughput.\n",
        "\n",
        "* **Coalesced Access**: When threads within the same warp access consecutive memory locations, the accesses can be coalesced into a single memory transaction, reducing memory latency and increasing bandwidth utilization.\n",
        "* **Data Alignment**: Ensuring that data is aligned to 32-byte or 128-byte boundaries (depending on the compute capability of the GPU) can further enhance the efficiency of memory accesses.\n"
      ],
      "metadata": {
        "id": "8P-rbOyE-IN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1r(float *A, float *B, float *C, int N) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N) { // Ensure the row is within the matrix\n",
        "        int index = row * N;\n",
        "        for (int col = 0; col < N; col++) {\n",
        "            A[index + col] = B[index + col] + C[index + col];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizes[] = {4, 16, 64, 256, 1024, 4096, 16384}; // Matrix sizes to test\n",
        "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
        "    int numRuns = 10;\n",
        "\n",
        "    for (int s = 0; s < numSizes; s++) {\n",
        "        int N = sizes[s];\n",
        "        size_t size = N * N * sizeof(float);\n",
        "        float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "        float totalMilliseconds = 0.0, avgMilliseconds;\n",
        "\n",
        "        // Allocate space for host copies of A, B, C and setup values\n",
        "        A = (float *)malloc(size); randomInit(A, N*N);\n",
        "        B = (float *)malloc(size); randomInit(B, N*N);\n",
        "        C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "        // Allocate space for device copies of A, B, C\n",
        "        cudaMalloc((void **)&d_A, size);\n",
        "        cudaMalloc((void **)&d_B, size);\n",
        "        cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "        // Copy inputs to device\n",
        "        cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Setup CUDA events for timing\n",
        "        cudaEvent_t start, end;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&end);\n",
        "\n",
        "        for (int i = 0; i < numRuns; i++) {\n",
        "            // Record event and launch kernel\n",
        "            cudaEventRecord(start);\n",
        "\n",
        "            // Launch kernel_1t1c() kernel on GPU\n",
        "            dim3 threadsPerBlock(1);\n",
        "            dim3 numBlocks(N);\n",
        "            kernel_1t1r<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "            // Stop recording\n",
        "            cudaEventRecord(end);\n",
        "            cudaEventSynchronize(end); // Wait for the stop event to complete\n",
        "\n",
        "            float milliseconds = 0;\n",
        "\n",
        "            // Calculate elapsed time\n",
        "            cudaEventElapsedTime(&milliseconds, start, end);\n",
        "            totalMilliseconds += milliseconds;\n",
        "        }\n",
        "\n",
        "        avgMilliseconds = totalMilliseconds / numRuns;\n",
        "        printf(\"N = %d: Average Time for kernel_1t1c over %d runs: %f ms\\n\", N, numRuns, avgMilliseconds);\n",
        "\n",
        "        // Wait for GPU to finish before accessing on host\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Print results or check output\n",
        "        printf(\"First 10 elements for N = %d:\\n\", N);\n",
        "        for (int i = 0; i < 10 && i < N; i++) { // Example of printing first 10 elements\n",
        "            printf(\"%f \", A[i]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "\n",
        "        // Cleanup\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(end);\n",
        "        cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "        free(A); free(B); free(C);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "fwtnDguLnsKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88720257-712e-4555-95f0-12709db75752"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 4: Average Time for kernel_1t1c over 10 runs: 0.032054 ms\n",
            "First 10 elements for N = 4:\n",
            "124.835159 101.332863 77.915482 113.125603 \n",
            "N = 16: Average Time for kernel_1t1c over 10 runs: 0.008710 ms\n",
            "First 10 elements for N = 16:\n",
            "74.517647 98.296608 18.021336 99.150787 143.830017 130.534576 104.634445 129.568481 97.333878 128.621811 \n",
            "N = 64: Average Time for kernel_1t1c over 10 runs: 0.014781 ms\n",
            "First 10 elements for N = 64:\n",
            "51.953617 89.961128 61.007092 38.918518 170.510712 138.459351 109.247101 99.387833 164.447388 99.482315 \n",
            "N = 256: Average Time for kernel_1t1c over 10 runs: 0.040493 ms\n",
            "First 10 elements for N = 256:\n",
            "40.772362 65.722710 128.192261 50.025650 61.464783 177.513412 120.943428 73.261261 154.609848 95.283554 \n",
            "N = 1024: Average Time for kernel_1t1c over 10 runs: 0.366890 ms\n",
            "First 10 elements for N = 1024:\n",
            "136.829529 122.516586 98.160301 84.390724 27.842970 93.901207 71.524582 172.090012 108.210281 102.350830 \n",
            "N = 4096: Average Time for kernel_1t1c over 10 runs: 5.231472 ms\n",
            "First 10 elements for N = 4096:\n",
            "64.705223 80.196503 103.708237 97.520508 100.835884 122.165680 100.934860 66.719604 95.009132 35.037598 \n",
            "N = 16384: Average Time for kernel_1t1c over 10 runs: 61.421917 ms\n",
            "First 10 elements for N = 16384:\n",
            "114.099922 113.655663 68.049171 76.889755 121.218437 21.092766 64.133835 69.955887 84.861740 133.791931 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of each kernel\n",
        "\n",
        "## `kernel_1t1r` (One Thread per Row)\n",
        "\n",
        "This configuration assigns one thread to handle the addition of all elements in a single row of matrices $B$ and $C$, storing the result in the corresponding row of output matrix $A$. Each thread computes an entire row, which means it iterates over all columns within that row to perform the addition.\n",
        "\n",
        "### Key characteristics\n",
        "* **Simplicity in control flow**: Each thread handles a single row, simplifying the control structure within the thread as it loops over columns.\n",
        "* **Reduced thread utilization**: Fewer threads are activated compared to `kernel_1t1e`, as only one thread is needed per row, which might be beneficial for smaller matrices or specific GPU architectures.\n",
        "\n",
        "### Launch configuration\n",
        "\n",
        "`kernel_1t1r`'s launch configuration typically involves fewer blocks since each block can handle rows, if needed.\n",
        "\n",
        "* **Threads per block**: Could be set to handle multiple rows per block depending on the matrix size, often kept at one thread per block if each thread handles an entire row.\n",
        "* **Blocks per grid**. Equals the number of rows in the matrix, ensuring that each row is handled by one thread.\n",
        "\n",
        "### Memory access pattern\n",
        "\n",
        "The memory access pattern here is strided, as each thread accesses elements that are spaced apart by the length of the rows (i.e. the matrix's width).\n",
        "\n",
        "* **Threads per block**: Threads access memory in a strided pattern, which can result in non-coalesced memory accesses, leading to inefficiencies.\n",
        "* **Cache utilization**: Poor cache utilization due to the strided access pattern, which might increase memory latency."
      ],
      "metadata": {
        "id": "k2NpWHXrDY_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void kernel_1t1c(float *A, float *B, float *C, int N) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (col < N) { // Ensure the column is within the matrix\n",
        "        for (int row = 0; row < N; row++) {\n",
        "            int index = row * N + col;\n",
        "            A[index] = B[index] + C[index];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void randomInit(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++)\n",
        "        data[i] = rand() / (float)RAND_MAX * 100.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizes[] = {4, 16, 64, 256, 1024, 4096, 16384}; // Matrix sizes to test\n",
        "    int numSizes = sizeof(sizes) / sizeof(sizes[0]);\n",
        "    int numRuns = 10;\n",
        "\n",
        "    for (int s = 0; s < numSizes; s++) {\n",
        "        int N = sizes[s];\n",
        "        size_t size = N * N * sizeof(float);\n",
        "        float *A, *B, *C, *d_A, *d_B, *d_C;\n",
        "        float totalMilliseconds = 0.0, avgMilliseconds;\n",
        "\n",
        "        // Allocate space for host copies of A, B, C and setup values\n",
        "        A = (float *)malloc(size); randomInit(A, N*N);\n",
        "        B = (float *)malloc(size); randomInit(B, N*N);\n",
        "        C = (float *)malloc(size); randomInit(C, N*N);\n",
        "\n",
        "        // Allocate space for device copies of A, B, C\n",
        "        cudaMalloc((void **)&d_A, size);\n",
        "        cudaMalloc((void **)&d_B, size);\n",
        "        cudaMalloc((void **)&d_C, size);\n",
        "\n",
        "        // Copy inputs to device\n",
        "        cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_C, C, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Setup CUDA events for timing\n",
        "        cudaEvent_t start, end;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&end);\n",
        "\n",
        "        for (int i = 0; i < numRuns; i++) {\n",
        "            // Record event and launch kernel\n",
        "            cudaEventRecord(start);\n",
        "\n",
        "            // Launch kernel_1t1c() kernel on GPU\n",
        "            dim3 threadsPerBlock(1);\n",
        "            dim3 numBlocks(N);\n",
        "            kernel_1t1c<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "            // Stop recording\n",
        "            cudaEventRecord(end);\n",
        "            cudaEventSynchronize(end); // Wait for the stop event to complete\n",
        "\n",
        "            float milliseconds = 0;\n",
        "\n",
        "            // Calculate elapsed time\n",
        "            cudaEventElapsedTime(&milliseconds, start, end);\n",
        "            totalMilliseconds += milliseconds;\n",
        "        }\n",
        "\n",
        "        avgMilliseconds = totalMilliseconds / numRuns;\n",
        "        printf(\"N = %d: Average Time for kernel_1t1c over %d runs: %f ms\\n\", N, numRuns, avgMilliseconds);\n",
        "\n",
        "        // Wait for GPU to finish before accessing on host\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(A, d_A, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Print results or check output\n",
        "        printf(\"First 10 elements for N = %d:\\n\", N);\n",
        "        for (int i = 0; i < 10 && i < N; i++) { // Example of printing first 10 elements\n",
        "            printf(\"%f \", A[i]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "\n",
        "        // Cleanup\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(end);\n",
        "        cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "        free(A); free(B); free(C);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "lSm3wHMptII0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f0cab5-5a26-4712-dc5c-9e488082b285"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 4: Average Time for kernel_1t1c over 10 runs: 0.029888 ms\n",
            "First 10 elements for N = 4:\n",
            "124.835159 101.332863 77.915482 113.125603 \n",
            "N = 16: Average Time for kernel_1t1c over 10 runs: 0.011264 ms\n",
            "First 10 elements for N = 16:\n",
            "74.517647 98.296608 18.021336 99.150787 143.830017 130.534576 104.634445 129.568481 97.333878 128.621811 \n",
            "N = 64: Average Time for kernel_1t1c over 10 runs: 0.029440 ms\n",
            "First 10 elements for N = 64:\n",
            "51.953617 89.961128 61.007092 38.918518 170.510712 138.459351 109.247101 99.387833 164.447388 99.482315 \n",
            "N = 256: Average Time for kernel_1t1c over 10 runs: 0.100413 ms\n",
            "First 10 elements for N = 256:\n",
            "40.772362 65.722710 128.192261 50.025650 61.464783 177.513412 120.943428 73.261261 154.609848 95.283554 \n",
            "N = 1024: Average Time for kernel_1t1c over 10 runs: 1.130259 ms\n",
            "First 10 elements for N = 1024:\n",
            "136.829529 122.516586 98.160301 84.390724 27.842970 93.901207 71.524582 172.090012 108.210281 102.350830 \n",
            "N = 4096: Average Time for kernel_1t1c over 10 runs: 15.772429 ms\n",
            "First 10 elements for N = 4096:\n",
            "64.705223 80.196503 103.708237 97.520508 100.835884 122.165680 100.934860 66.719604 95.009132 35.037598 \n",
            "N = 16384: Average Time for kernel_1t1c over 10 runs: 165.615112 ms\n",
            "First 10 elements for N = 16384:\n",
            "114.099922 113.655663 68.049171 76.889755 121.218437 21.092766 64.133835 69.955887 84.861740 133.791931 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of each kernel\n",
        "\n",
        "## `kernel_1t1c` (One Thread per Column)\n",
        "\n",
        "Here, each thread is responsible for processing all the elements in a single column of matrices $B$ and $C$, with the results stored in the corresponding column of matrix $A$. The thread iterates over all rows for its assigned column to perform the addition.\n",
        "\n",
        "### Key characteristics\n",
        "* **Column-based focus**: Suited for operations that are inherently columnar in nature, potentially aligning better with subsequent column-based processing steps.\n",
        "* **Reduced thread utilization**: Similar to `kernel_1t1e`, this approach activates fewer threads, which might be more efficient for specific GPU architectures or smaller matrices.\n",
        "\n",
        "### Launch configuration\n",
        "\n",
        "`kernel_1t1c`'s configuration typically involves one thread per column, which might lead to using a different number of threads per block, compared to `kerne_1t1r`.\n",
        "\n",
        "* **Threads per block**: Commonly set as one thread per block if each thread handles an entire column.\n",
        "* **Blocks per grid**. Set to match the number of columns in the matrix, ensuring that each column is processed by one thread.\n",
        "\n",
        "### Memory access pattern\n",
        "\n",
        "This pattern is also strided but in a transposed manner to `kernel_1t1r`.\n",
        "\n",
        "* **Non-Coalesced Access**: Threads access matrix elements per column, which leads to a strided access pattern similar to `kernel_1t1r`, resulting in non-coalesced accesses.\n",
        "* **Cache utilization**: Similar poor cache utilization due to the vertical strided pattern, potentially increasing memory latency and reducing throughput."
      ],
      "metadata": {
        "id": "-5myXbNTOrH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Both `kernel_1t1r` and `kernel_1t1c` provide specific benefits for particular problem domains but tend to be less efficient than `kernel_1t1e` in terms of memory access and overall GPU resource utilization. These kernels might be preferred when subsequent operations in the computation pipeline benefit from having an entire row of column processed by a single thread, or when experimenting with memory traffic patterns on specific GPU architectures.\n",
        "\n",
        "## Pros and cons\n",
        "\n",
        "### `kernel_1t1e`\n",
        "\n",
        "* **Pro**: Coalesced memory access. If aligned properly with memory boundaries, this kernel allows for coalesced memory accesses, where consecutive threads access consecutive memory addresses, leading to optimal use of the memory bandwidth.\n",
        "* **Pro**: Load balancing. Every thread does exactly the same amount of work, which maximizes parallel efficiency and minimizes idle time for compute units.\n",
        "* **Cons**: Potential for overhead. If the matrix is very large (which I did not show in this paper), the great number of threads can cause extra overhead due to thread management.\n",
        "\n",
        "### `kernel_1t1r`\n",
        "\n",
        "* **Pro**: Simple indexing. This method simplifies indexing logic, as each thread handles a complete row, which made it easier for me to manage the row additions.\n",
        "* **Pro**: Reduced number of threads. Fewer threads felt like it was less of an overhead given my test data, which were essentially just small matrices.\n",
        "* **Con**: Non-coalesced Memory Address. Threads access memory in a strided pattern, which can result in poor utilization of the memory bus and lower performance.\n",
        "* **Con**: Load imbalance. In matrices where the number of columsn is not a power of two, or does not match well with the number of CUDA cores, some threads might be underutilized (CITE T4!)\n",
        "\n",
        "### `kernel_1t1c`\n",
        "\n",
        "* **Pro**: Suitable for column-based operations. It can be efficient if the subsequent operations or algorithms to be used are column-based, aligning better with certain linear algebra operations.\n",
        "* **Cons**: Poor memory access pattern. Similar to `kernel_1t1r`, this approach suffers from strided memory access, which is inefficient on GPUs due to non-coalesced accesses.\n",
        "* **Cons**: Load Imbalance. Like the row-wise kernel, the distribution of work might not be optimal, especially in wide or very narrow matrices.\n"
      ],
      "metadata": {
        "id": "BNd0HPO_V23d"
      }
    }
  ]
}